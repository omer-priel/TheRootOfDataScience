{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "\n",
    "# Web Scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# View\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches\n",
    "import seaborn as sns\n",
    "from IPython.display import Image\n",
    "\n",
    "# ML\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "# ML Models\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Utilities Files\n",
    "def read_csv(name: str, index_label='id') -> pd.DataFrame:\n",
    "    return pd.read_csv('../data/' + name + '.csv', index_col=index_label)\n",
    "\n",
    "\n",
    "def save_csv(df: pd.DataFrame, name: str, index_label='id'):\n",
    "    df.to_csv('../data/' + name + '.csv', index_label=index_label)\n",
    "\n",
    "def show_image(name: str):\n",
    "    return Image(filename= '../images/' + name + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Development Flags\n",
    "SHOW_IN_RELEASE = False\n",
    "SHOW_IN_DEVELOPMENT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "show_image('Sorry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "show_image('Data_Collectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_original = read_csv('businesses')\n",
    "df_original = df_original.head(5000)\n",
    "\n",
    "df = df_original.copy()\n",
    "\n",
    "df.drop([\"Url\", \"Name\", \"ExpensiveLevel\", \"SubCategories\", \"AttributesHas\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Y = target = df[\"HasExpensiveLevel\"]\n",
    "X = data = df.drop([\"HasExpensiveLevel\"], axis=1)\n",
    "x_train, x_finale_test, y_train, y_finale_test = train_test_split(X, Y, test_size=0.03, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = df_train = pd.concat([y_train, x_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Visuation for undsending the data\n",
    "\n",
    "# Config\n",
    "legend_colors = ['tab:blue', 'tab:orange']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target_column = \"HasExpensiveLevel\"\n",
    "target_column_label = \"Is Expensive\"\n",
    "target_column_label_true = \"Is Expensive\"\n",
    "target_column_label_false = \"Is not Expensive\"\n",
    "\n",
    "prime_flag_columns = [\n",
    "    'Claimed', 'HasWebsite'\n",
    "]\n",
    "\n",
    "prime_countable_columns = [\n",
    "    'Stars',\n",
    "    'SubCategoriesCount', 'AttributesCount',\n",
    "    'QuestionsCount', 'WeeklyBreaks', 'WeeklyDays'\n",
    "]\n",
    "prime_non_countable_columns = [\n",
    "     'Reviews', 'Photos',\n",
    "    'WeeklyHours'\n",
    "]\n",
    "\n",
    "prime_columns = prime_flag_columns + prime_countable_columns + prime_non_countable_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numberic Visuation\n",
    "df[prime_columns + [target_column]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visuation per column and between columns\n",
    "\n",
    "def disply_count_of_flag_column(column_name: str, column_label: str, true_label: str, false_label: str, labels: list):\n",
    "    global legend_colors, df, target_column\n",
    "    \n",
    "    df_sp = df[[target_column, column_name]]\n",
    "    \n",
    "    count_df = df_sp.groupby([column_name]).count()\n",
    "    \n",
    "    count_true_df = df_sp[df_sp[column_name] == 1].groupby([target_column]).count()\n",
    "    count_false_df = df_sp[df_sp[column_name] == 0].groupby([target_column]).count()\n",
    "    \n",
    "    # Main elements    \n",
    "    fig = plt.figure(figsize = [16, 8])\n",
    "    \n",
    "    fig.suptitle(column_label)\n",
    "    \n",
    "    ax_graf =  fig.add_subplot(1, 3, 1)\n",
    "    ax_true =  fig.add_subplot(1, 3, 2)\n",
    "    ax_false = fig.add_subplot(1, 3, 3) \n",
    "        \n",
    "    # Graf\n",
    "    plt.sca(ax_graf)\n",
    "    ax_graf.bar( [true_label,false_label], count_df[target_column].values)\n",
    "    plt.xlabel(true_label)\n",
    "    plt.ylabel(\"Count\")\n",
    "    \n",
    "    # Pies\n",
    "    plt.sca(ax_true)\n",
    "    plt.title(true_label)\n",
    "    \n",
    "    ax_true.pie(count_true_df[column_name].values,\n",
    "                radius=4,\n",
    "                center=(4, 4),\n",
    "                colors=legend_colors)\n",
    "    \n",
    "    ax_true.set(xlim=(0, 8), ylim=(0, 8))\n",
    "        \n",
    "    plt.sca(ax_false)\n",
    "    plt.title(false_label)\n",
    "    ax_false.pie(count_false_df[column_name].values,\n",
    "                radius=4,\n",
    "                center=(4, 4),\n",
    "                colors=legend_colors)\n",
    "    \n",
    "    ax_false.set(xlim=(0, 8), ylim=(0, 8))\n",
    "    \n",
    "    # show        \n",
    "    handles = []\n",
    "    for i in range(len(labels)):\n",
    "        handles.append(matplotlib.patches.Patch(label=labels[i], color=legend_colors[i]))\n",
    "\n",
    "    fig.legend(handles=handles, loc =\"lower right\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disply_atter_per_column(inedexs: list, values: list, columns_label: list):\n",
    "    global legend_colors, target_column, target_column_label\n",
    "    \n",
    "    # Main elements    \n",
    "    fig = plt.figure(figsize = [16, 8])\n",
    "        \n",
    "    for i in range(len(values)):        \n",
    "        value = values[i]\n",
    "        if value is not None:\n",
    "            index = indexs[i]\n",
    "            label = columns_label[i]\n",
    "                    \n",
    "            # Graf\n",
    "            ax_graf =  fig.add_subplot(1, len(values), i+1)\n",
    "        \n",
    "            plt.sca(ax_graf)\n",
    "            plt.plot(index, value)\n",
    "            plt.xlabel(label)\n",
    "            plt.ylabel(target_column_label)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disply_mluti_bars(inedexs: list, values: list, indexs_label, values_label, w: int):\n",
    "    global legend_colors\n",
    "    \n",
    "    values += np.full(elements_in_line-len(values)%elements_in_line, None).tolist()\n",
    "    inedexs += np.full(elements_in_line-len(values)%elements_in_line, None).tolist()\n",
    "    \n",
    "    h = int(len(values) / elements_in_line)\n",
    "    \n",
    "    # Main elements \n",
    "    fig = plt.figure(figsize = [16, h * 8])\n",
    "    \n",
    "    gs = fig.add_gridspec(h, w)\n",
    "    \n",
    "    line = 0\n",
    "    while len(values) > 0:\n",
    "        now_values = values[:elements_in_line]\n",
    "        now_inedexs = inedexs[:elements_in_line]\n",
    "                \n",
    "        for i in range(w):        \n",
    "            value = now_values[i]\n",
    "            if value is not None:\n",
    "                index = now_inedexs[i]\n",
    "                value = now_values[i]\n",
    "            \n",
    "                index_label = indexs_label\n",
    "                value_label = values_label\n",
    "                if type(indexs_label) == list:\n",
    "                    index_label = indexs_label[i]\n",
    "            \n",
    "                if type(values_label) == list:\n",
    "                    value_label = values_label[i]\n",
    "            \n",
    "                # Graf\n",
    "                ax_graf = fig.add_subplot(gs[line, i])\n",
    "        \n",
    "                plt.sca(ax_graf)\n",
    "                plt.bar(index, value)\n",
    "                plt.xlabel(index_label)\n",
    "                plt.ylabel(value_label)\n",
    "                \n",
    "        values = values[elements_in_line:]\n",
    "        inedexs = inedexs[elements_in_line:]\n",
    "        \n",
    "        if type(indexs_label) == list:\n",
    "            indexs_label = indexs_label[elements_in_line:]\n",
    "            \n",
    "        if type(values_label) == list:\n",
    "            values_label = values_label[elements_in_line:]\n",
    "        \n",
    "        line += 1\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Shows the average of \"Has Expensive Level\" for every column by value\n",
    "# Average of \"Has Expensive Level\" symbolizes how likely it is to have expensive level\n",
    "\n",
    "if SHOW_IN_DEVELOPMENT:\n",
    "    \n",
    "    printed_columns = prime_countable_columns\n",
    "    atter_name = 'mean'\n",
    "    \n",
    "    elements_in_line = 3\n",
    "    printed_columns += np.full(elements_in_line-len(printed_columns)%elements_in_line, None).tolist()\n",
    "    while len(printed_columns) > 0:\n",
    "        now_columns = printed_columns[:elements_in_line]\n",
    "        indexs = []\n",
    "        values = []\n",
    "        for column in now_columns:\n",
    "            if column is None:\n",
    "                indexs += [None]\n",
    "                values += [None]\n",
    "            else:\n",
    "                atter_df = df[[target_column, column]].groupby([column])\n",
    "                atter_df = getattr(atter_df, atter_name)()\n",
    "            \n",
    "                indexs += [atter_df.index]\n",
    "                values += [atter_df[target_column].values]            \n",
    "        \n",
    "        disply_atter_per_column(indexs, values, printed_columns[:elements_in_line])\n",
    "        printed_columns = printed_columns[elements_in_line:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculates the probability based on sections\n",
    "\n",
    "if SHOW_IN_DEVELOPMENT:\n",
    "    \n",
    "    sections=20\n",
    "    atter_name = 'mean'\n",
    "    elements_in_line = 2\n",
    "    printed_columns = prime_non_countable_columns\n",
    "    \n",
    "    printed_columns += np.full(elements_in_line-len(printed_columns)%elements_in_line, None).tolist()\n",
    "    while len(printed_columns) > 0:\n",
    "        now_columns = printed_columns[:elements_in_line]\n",
    "        indexs = []\n",
    "        values = []\n",
    "        for column in now_columns:\n",
    "            if column is None:\n",
    "                indexs += [None]\n",
    "                values += [None]\n",
    "            else:\n",
    "                new_df = df[[target_column, column]].sort_values(column)\n",
    "                length = len(new_df)\n",
    "                new_df[\"Sections\"] = df.index//(length//sections)\n",
    "                section_name = range(0,100,100//sections)\n",
    "                atter_df = new_df[[target_column, \"Sections\"]].groupby([\"Sections\"])\n",
    "                atter_df = getattr(atter_df, atter_name)()\n",
    "            \n",
    "                indexs += [atter_df.index]\n",
    "                values += [atter_df[target_column].values]            \n",
    "        \n",
    "        disply_atter_per_column(indexs, values, printed_columns[:elements_in_line])\n",
    "        printed_columns = printed_columns[elements_in_line:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows Claimed\n",
    "\n",
    "if SHOW_IN_DEVELOPMENT: \n",
    "    disply_count_of_flag_column('Claimed', 'Claimed', 'Claimed', 'Unclaimed',\n",
    "                                ['Has Expensive Level', 'Not has Expensive Level'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Has Website\n",
    "\n",
    "if SHOW_IN_DEVELOPMENT:\n",
    "    disply_count_of_flag_column('HasWebsite', 'Has Website', 'Have', 'Not Have',\n",
    "                                [target_column_label_true, target_column_label_false])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Shows how much data we have for each value in each feature\n",
    "\n",
    "if SHOW_IN_DEVELOPMENT:\n",
    "    \n",
    "    printed_columns = prime_countable_columns  \n",
    "    elements_in_line = 3\n",
    "    indexs = []\n",
    "    values = []\n",
    "    for column in printed_columns:\n",
    "        if column is not None:\n",
    "            count_df = df[[target_column, column]].groupby([column]).count()\n",
    "            \n",
    "            indexs += [count_df.index]\n",
    "            values += [count_df[target_column].values.tolist()]            \n",
    "    \n",
    "    disply_mluti_bars(indexs, values, printed_columns, 'Count', elements_in_line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_IN_DEVELOPMENT:\n",
    "    \n",
    "    printed_columns = prime_countable_columns + prime_non_countable_columns    \n",
    "    elements_in_line = 3\n",
    "    printed_columns += np.full(elements_in_line-len(printed_columns)%elements_in_line, None).tolist()\n",
    "    while len(printed_columns) > 0:\n",
    "        now_columns = printed_columns[:elements_in_line]\n",
    "        indexs = []\n",
    "        values = []\n",
    "        for column in now_columns:\n",
    "            if column is None:\n",
    "                indexs += [None]\n",
    "                values += [None]\n",
    "            else:\n",
    "                count_df = df[[target_column, column]].groupby([column]).count()\n",
    "            \n",
    "                indexs += [count_df.index]\n",
    "                values += [count_df[target_column].values.tolist()]            \n",
    "\n",
    "        disply_mluti_bars(indexs, values, printed_columns[:elements_in_line], 'Count', elements_in_line)\n",
    "        printed_columns = printed_columns[elements_in_line:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##shows how much features we lose if we decide to limit the amount of instances a category should have to appear\n",
    "if SHOW_IN_DEVELOPMENT:\n",
    "    column_values = df.columns.map(lambda x: x.startswith(\"Cat_\"))\n",
    "    cat_df = df.loc[:, column_values]\n",
    "    categ = cat_df.sum().sort_values()\n",
    "    limits = range(50)\n",
    "    remainders = []\n",
    "    for limit in limits:\n",
    "        remainders += [(categ.values<limit).sum()]\n",
    "    fig= plt.figure()\n",
    "    ax_graph= fig.add_subplot(1,1,1)\n",
    "    ax_graph= plt.step(remainders,limits)\n",
    "    for i in range(0,50,5):\n",
    "        plt.plot([0,300],[i,i],linestyle= \":\" )\n",
    "    plt.ylabel(\"the cutoff\")\n",
    "    plt.xlabel(\"categories we lose\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##shows us how much categories we have for different probabilities  \n",
    "if SHOW_IN_DEVELOPMENT:\n",
    "    sub_categories = filter(lambda x: x.startswith(\"Cat_\"),df.columns)\n",
    "    x0 = np.arange(0,1.05,0.05)\n",
    "    y0 = np.zeros(21)\n",
    "    x1 = np.arange(0,1.05,0.05)\n",
    "    y1 = np.zeros(21)\n",
    "    for category in sub_categories:\n",
    "        mean_df = df[[target_column, category]].groupby([category]).mean()\n",
    "        y0[(int)((mean_df.loc[0.0,target_column]*20).round())] += 1\n",
    "        if (1.0 in mean_df.index):\n",
    "            y1[(int)((mean_df.loc[1.0,target_column]*20).round())] += 1\n",
    "    fig = plt.figure()\n",
    "    ax0_graph = fig.add_subplot(2,1,1)\n",
    "    plt.plot(x0,y0)\n",
    "    plt.xlabel(\"probability of is expensive given not in category\")\n",
    "    plt.ylabel(\"amount of categories\")\n",
    "    ax1_graph = fig.add_subplot(2,1,2)\n",
    "    ax1_graph = plt.plot(x1,y1)\n",
    "    plt.xlabel(\"probability of is expensive given in category\")\n",
    "    plt.ylabel(\"amount of categories\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create collumns which generally represents a bunch of other collumns based on their relative probability\n",
    "class create_general_collumns:\n",
    "    collumn_map = np.empty(16, dtype= object)\n",
    "    \n",
    "    def __init__(self,starter):\n",
    "        self.starts = lambda x:x.startswith(starter)\n",
    "        \n",
    "        \n",
    "    def fit(self,df_create:pd.DataFrame):\n",
    "        sub_categories = filter(self.starts,df_create.columns)\n",
    "        np.empty(16, dtype= object)\n",
    "        for i in range(self.collumn_map.shape[0]):\n",
    "            self.collumn_map[i] = []\n",
    "        for category in sub_categories:\n",
    "            mean_df = df[[target_column, category]].groupby([category]).mean()\n",
    "            if (1.0 in mean_df.index):\n",
    "                self.collumn_map[(int)((mean_df.loc[1.0,target_column]*15).round())] += [category]\n",
    "    \n",
    "    \n",
    "    def transform(self,df_create:pd.DataFrame):\n",
    "        index=0\n",
    "        for category_class in self.collumn_map:\n",
    "            df_create[\"category_class\"+str(index)] = df_create[category_class].sum(axis=1)\n",
    "            index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_IN_DEVELOPMENT:\n",
    "    week_days={\"2\":\"Monday\",\"3\":\"Tuesday\",\"4\":\"Wednesday\",\"5\": \"Thursday\" , \"6\":\"Friday\", \"7\":\"Saturday\",\"1\":\"Sunday\" }\n",
    "    \n",
    "    time_open_select = [\n",
    "    '12:00 AM', '12:30 AM',\n",
    "    '1:00 AM', '1:30 AM',\n",
    "    '2:00 AM', '2:30 AM',\n",
    "    '3:00 AM', '3:30 AM',\n",
    "    '4:00 AM', '4:30 AM',\n",
    "    '5:00 AM', '5:30 AM',\n",
    "    '6:00 AM', '6:30 AM',\n",
    "    '7:00 AM', '7:30 AM',\n",
    "    '8:00 AM', '8:30 AM',\n",
    "    '9:00 AM', '9:30 AM',\n",
    "    '10:00 AM', '10:30 AM',\n",
    "    '11:00 AM', '11:30 AM',\n",
    "    '12:00 PM', '12:30 PM',\n",
    "    '1:00 PM', '1:30 PM',\n",
    "    '2:00 PM', '2:30 PM',\n",
    "    '3:00 PM', '3:30 PM',\n",
    "    '4:00 PM', '4:30 PM',\n",
    "    '5:00 PM', '5:30 PM',\n",
    "    '6:00 PM', '6:30 PM',\n",
    "    '7:00 PM', '7:30 PM',\n",
    "    '8:00 PM', '8:30 PM',\n",
    "    '9:00 PM', '9:30 PM',\n",
    "    '10:00 PM', '10:30 PM',\n",
    "    '11:00 PM', '11:30 PM',\n",
    "    ]\n",
    "\n",
    "    time_end_select = [\n",
    "    '12:00 AM', '12:30 AM',\n",
    "    '1:00 AM', '1:30 AM',\n",
    "    '2:00 AM', '2:30 AM',\n",
    "    '3:00 AM', '3:30 AM',\n",
    "    '4:00 AM', '4:30 AM',\n",
    "    '5:00 AM', '5:30 AM',\n",
    "    '6:00 AM', '6:30 AM',\n",
    "    '7:00 AM', '7:30 AM',\n",
    "    '8:00 AM', '8:30 AM',\n",
    "    '9:00 AM', '9:30 AM',\n",
    "    '10:00 AM', '10:30 AM',\n",
    "    '11:00 AM', '11:30 AM',\n",
    "    '12:00 PM', '12:30 PM',\n",
    "    '1:00 PM', '1:30 PM',\n",
    "    '2:00 PM', '2:30 PM',\n",
    "    '3:00 PM', '3:30 PM',\n",
    "    '4:00 PM', '4:30 PM',\n",
    "    '5:00 PM', '5:30 PM',\n",
    "    '6:00 PM', '6:30 PM',\n",
    "    '7:00 PM', '7:30 PM',\n",
    "    '8:00 PM', '8:30 PM',\n",
    "    '9:00 PM', '9:30 PM',\n",
    "    '10:00 PM', '10:30 PM',\n",
    "    '11:00 PM', '11:30 PM',\n",
    "    '12:00 AM ', '12:30 AM',\n",
    "    '1:00 AM ', '1:30 AM ',\n",
    "    '2:00 AM ', '2:30 AM ',\n",
    "    '3:00 AM ', '3:30 AM ',\n",
    "    '4:00 AM ', '4:30 AM ',\n",
    "    '5:00 AM ', '5:30 AM ',\n",
    "    '6:00 AM ', '6:30 AM ',\n",
    "    ]\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize= (16,16))\n",
    "    ax_open = fig.add_subplot(5,2,1)    \n",
    "    ax_count = fig.add_subplot(5,2,2)\n",
    "    \n",
    "    #handle the opening hour\n",
    "    plt.sca(ax_open)\n",
    "    plt.ylabel(\"is expensive\")\n",
    "    plt.xlabel(\"Opening Hour\")\n",
    "    for i in range(7):\n",
    "        mean_df = df[[\"OpenHour\"+str(i+1),target_column]].groupby([\"OpenHour\"+str(i+1)]).mean()\n",
    "        plt.plot(mean_df.index,mean_df.values,label = week_days[str(i+1)])\n",
    "    plt.legend()\n",
    "    ax_open.set_xticks(range(0,48,6))\n",
    "    ax_open.set_xticklabels(map(lambda x:time_open_select[int(x)], range(0,48,6)))\n",
    "    \n",
    "    #handle count hour\n",
    "    plt.sca(ax_count)\n",
    "    plt.ylabel(\"is expensive\")\n",
    "    plt.xlabel(\"daily Hours\")\n",
    "    for i in range(7):\n",
    "        mean_df = df[[\"CountHour\"+str(i+1),target_column]].groupby([\"CountHour\"+str(i+1)]).mean()\n",
    "        plt.plot(mean_df.index,mean_df.values,label = week_days[str(i+1)])\n",
    "    plt.legend()\n",
    "    \n",
    "    #handle the ending hour\n",
    "    for i in range(7):\n",
    "        ax_end=fig.add_subplot(5,2,3+i)\n",
    "        plt.sca(ax_end)\n",
    "        plt.ylabel(\"is expensive\")\n",
    "        plt.xlabel(\"ending Hour\")\n",
    "        ax_end.set_title(week_days[str(i+1)])\n",
    "        mean_df = df[[\"EndHour\"+str(i+1),target_column]].groupby([\"EndHour\"+str(i+1)]).mean()\n",
    "        plt.plot(mean_df.index,mean_df.values)\n",
    "        ax_end.set_xticks(range(0,61,6))\n",
    "        ax_end.set_xticklabels(map(lambda x:time_end_select[int(x)], range(0,61,6)))\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_sub_train, x_sub_test, y_sub_train, y_sub_test = train_test_split(x_train, y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Simple model base KNN\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(x_sub_train, y_sub_train)\n",
    "\n",
    "score = knn.score(x_sub_test, y_sub_test)\n",
    "print(\"Score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the minimum score\n",
    "\n",
    "dummy_modal = DummyClassifier()\n",
    "\n",
    "dummy_modal.fit(x_sub_train, y_sub_train)\n",
    "\n",
    "score = dummy_modal.score(x_sub_test, y_sub_test)\n",
    "print(\"Dummy Modal Score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_models = pd.DataFrame({\n",
    "    'Name': [],\n",
    "    'Score': []\n",
    "})\n",
    "\n",
    "def try_multi_models(models):\n",
    "    global x_sub_train, y_sub_train, x_sub_test, y_sub_test\n",
    "    global df_models\n",
    "    \n",
    "    df_models_add = pd.DataFrame({\n",
    "    'Name': np.full(len(models), None),\n",
    "    'Score': np.full(len(models), None)\n",
    "    })\n",
    "        \n",
    "    i = 0\n",
    "    for model in models:\n",
    "        df_models_add.at[i, 'Name'] = model['name']\n",
    "        try:\n",
    "            model['model'].fit(x_sub_train, y_sub_train)\n",
    "            \n",
    "            score = model['model'].score(x_sub_test, y_sub_test)\n",
    "            df_models_add.at[i, 'Score'] = score\n",
    "            print(model['name'] + \" Score: \", score)\n",
    "        except:\n",
    "            print(model['name'] + \" Failed!\")\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    df_models = pd.concat([df_models, df_models_add])\n",
    "    df_models.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First tring\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        'name': 'KNeighborsClassifier',\n",
    "        'model': KNeighborsClassifier()\n",
    "    }, {\n",
    "        'name': 'LogisticRegression',\n",
    "        'model': LogisticRegression()\n",
    "    }, {\n",
    "        'name': 'Lasso',\n",
    "        'model': Lasso()\n",
    "    }, {\n",
    "        'name': 'SVC',\n",
    "        'model': SVC()\n",
    "    }, {\n",
    "        'name': 'LinearSVC',\n",
    "        'model': LinearSVC()\n",
    "    }, {\n",
    "        'name': 'RandomForestClassifier',\n",
    "        'model': RandomForestClassifier()\n",
    "    }\n",
    "]\n",
    "\n",
    "try_multi_models(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Try svm models\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        'name': 'OneClassSVM',\n",
    "        'model': OneClassSVM(gamma='auto')\n",
    "    }, {\n",
    "        'name': 'SVR',\n",
    "        'model': SVR()\n",
    "    }\n",
    "]\n",
    "\n",
    "try_multi_models(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Try models like Random Forest\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        'name': 'RandomForestRegressor',\n",
    "        'model': RandomForestRegressor()\n",
    "    }\n",
    "]\n",
    "\n",
    "try_multi_models(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models.sort_values('Score', ascending=False, inplace=True)\n",
    "df_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bast score comes form Random Forest Classifier\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Learning about VotingClassifier\n",
    "\n",
    "vs = VotingClassifier(estimators=[\n",
    "    ('RandomForestClassifier', RandomForestClassifier()),\n",
    "    ('LogisticRegression', LogisticRegression())\n",
    "])\n",
    "\n",
    "vs.fit(x_sub_train, y_sub_train)\n",
    "\n",
    "score = vs.score(x_sub_test, y_sub_test)\n",
    "print(\"Voting Classifier Score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=1)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 500,300,400,600,700],\n",
    "    \n",
    "}\n",
    "grid_search = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "\n",
    "grid_search.fit(x_sub_train, y_sub_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "score = grid_search.score(x_sub_test, y_sub_test)\n",
    "print(\"Score: \", score)\n",
    "\n",
    "res_as_df = pd.concat([pd.DataFrame(grid_search.cv_results_[\"params\"]), pd.DataFrame(grid_search.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])], axis=1)\n",
    "res_as_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_sub_train=scaler.fit_transform(X=x_sub_train)\n",
    "x_sub_test=scaler.transform(x_sub_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    {\n",
    "        'name': 'KNeighborsClassifier',\n",
    "        'model': KNeighborsClassifier()\n",
    "    }, {\n",
    "        'name': 'LogisticRegression',\n",
    "        'model': LogisticRegression()\n",
    "    }, {\n",
    "        'name': 'Lasso',\n",
    "        'model': Lasso()\n",
    "    }, {\n",
    "        'name': 'SVC',\n",
    "        'model': SVC()\n",
    "    }, {\n",
    "        'name': 'LinearSVC',\n",
    "        'model': LinearSVC()\n",
    "    }, {\n",
    "        'name': 'RandomForestClassifier',\n",
    "        'model': RandomForestClassifier()\n",
    "    }\n",
    "]\n",
    "\n",
    "try_multi_models(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
